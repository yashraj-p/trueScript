{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "8Z33feclY2Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/new_dataset.csv')\n",
        "\n",
        "# Check for NaN values\n",
        "print(\"NaN values in the dataset:\")\n",
        "print(data.isna().sum())\n",
        "\n",
        "# Remove rows with NaN values\n",
        "data_cleaned = data.dropna()"
      ],
      "metadata": {
        "id": "vCTN9-SNY2eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the features and labels\n",
        "numerical_features = ['word density', 'avg line length', 'mean_perplexity', 'burstiness1', 'flesch_kincaid_score', 'gunning_fog_score', 'burstiness2']\n",
        "X_numerical = data_cleaned[numerical_features].values\n",
        "X_text = data_cleaned['text'].values\n",
        "y = data_cleaned['label'].values\n",
        "\n",
        "# Split the data into train+val and test sets\n",
        "X_text_trainval, X_text_test, X_num_trainval, X_num_test, y_trainval, y_test = train_test_split(\n",
        "    X_text, X_numerical, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "pOcnxmOvY2jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "max_words = 10000\n",
        "max_length = 350\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_text_trainval)\n",
        "X_text_trainval_seq = tokenizer.texts_to_sequences(X_text_trainval)\n",
        "X_text_test_seq = tokenizer.texts_to_sequences(X_text_test)\n",
        "X_text_trainval_padded = pad_sequences(X_text_trainval_seq, maxlen=max_length)\n",
        "X_text_test_padded = pad_sequences(X_text_test_seq, maxlen=max_length)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_num_trainval_scaled = scaler.fit_transform(X_num_trainval)\n",
        "X_num_test_scaled = scaler.transform(X_num_test)"
      ],
      "metadata": {
        "id": "k5T9CYgjY2pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN-BiLSTM model\n",
        "def create_model(max_words, max_length, num_numerical_features):\n",
        "    # Text input\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    embedding = Embedding(max_words, 100)(text_input)\n",
        "\n",
        "    conv1 = Conv1D(64, 5, activation='relu')(embedding)\n",
        "    pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
        "    conv2 = Conv1D(128, 5, activation='relu')(pool1)\n",
        "    pool2 = MaxPooling1D(pool_size=4)(conv2)\n",
        "\n",
        "    bilstm = Bidirectional(LSTM(64, return_sequences=False))(pool2)\n",
        "\n",
        "    # Numerical input\n",
        "    numerical_input = Input(shape=(num_numerical_features,))\n",
        "\n",
        "    # Concatenate BiLSTM output and numerical features\n",
        "    combined = Concatenate()([bilstm, numerical_input])\n",
        "\n",
        "    x = Dense(64, activation='relu')(combined)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=[text_input, numerical_input], outputs=output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "YRGnv6uKY2t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up K-Fold cross-validation\n",
        "n_folds = 5\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_histories = []\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_model = None\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_text_trainval_padded), 1):\n",
        "    print(f\"Training on fold {fold}/{n_folds}...\")\n",
        "\n",
        "    X_text_train, X_text_val = X_text_trainval_padded[train_index], X_text_trainval_padded[val_index]\n",
        "    X_num_train, X_num_val = X_num_trainval_scaled[train_index], X_num_trainval_scaled[val_index]\n",
        "    y_train, y_val = y_trainval[train_index], y_trainval[val_index]\n",
        "\n",
        "    learning_rate = 2e-3\n",
        "\n",
        "    model = create_model(max_words, max_length, X_num_trainval.shape[1])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_text_train, X_num_train], y_train,\n",
        "        validation_data=([X_text_val, X_num_val], y_val),\n",
        "        epochs=3,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict([X_text_val, X_num_val])\n",
        "    y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "    fold_accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "    fold_accuracies.append(fold_accuracy)\n",
        "    fold_histories.append(history)\n",
        "\n",
        "    print(f\"Fold {fold} accuracy: {fold_accuracy:.4f}\")\n",
        "    print(classification_report(y_val, y_pred_classes))\n",
        "    print(confusion_matrix(y_val, y_pred_classes))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Save the best model\n",
        "    if fold_accuracy > best_accuracy:\n",
        "        best_accuracy = fold_accuracy\n",
        "        best_model = model\n",
        "\n",
        "best_model.save('best_cnn_bilstm_model.h5')\n",
        "print(\"Best model saved to 'best_cnn_bilstm_model.h5'\")\n",
        "\n",
        "print(f\"Average accuracy across all folds: {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Standard deviation of accuracy: {np.std(fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "t3jD7ZuMYitM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot k-fold cross-validation results\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "#accuracy\n",
        "plt.subplot(2, 1, 1)\n",
        "for i, history in enumerate(fold_histories):\n",
        "    plt.plot(history.history['accuracy'], label=f'Train (Fold {i+1})')\n",
        "    plt.plot(history.history['val_accuracy'], label=f'Validation (Fold {i+1})')\n",
        "plt.title('Model Accuracy Across Folds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "#loss\n",
        "plt.subplot(2, 1, 2)\n",
        "for i, history in enumerate(fold_histories):\n",
        "    plt.plot(history.history['loss'], label=f'Train (Fold {i+1})')\n",
        "    plt.plot(history.history['val_loss'], label=f'Validation (Fold {i+1})')\n",
        "plt.title('Model Loss Across Folds')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#final k-fold results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_folds + 1), fold_accuracies, marker='o')\n",
        "plt.title('Model Accuracy for Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "for i, acc in enumerate(fold_accuracies):\n",
        "    plt.text(i + 1, acc, f'{acc:.4f}', ha='center', va='bottom')\n",
        "plt.ylim(min(fold_accuracies) - 0.05, max(fold_accuracies) + 0.05)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEvaluating the best model on the test set:\")\n",
        "y_test_pred = best_model.predict([X_text_test_padded, X_num_test_scaled])\n",
        "y_test_pred_classes = (y_test_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nEvaluating the best model on the test set:\")\n",
        "test_loss, test_accuracy = best_model.evaluate([X_text_test_padded, X_num_test_scaled], y_test, verbose=0)\n",
        "y_test_pred = best_model.predict([X_text_test_padded, X_num_test_scaled])\n",
        "y_test_pred_classes = (y_test_pred > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "RT1UboIxYiyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "test_f1 = f1_score(y_test, y_test_pred_classes)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred_classes))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred_classes))\n",
        "\n",
        "# Plot ROC curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {test_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MV5i5I4lYjQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot test results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Test Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(['Test Accuracy'], [test_accuracy])\n",
        "plt.title('Test Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "for i, v in enumerate([test_accuracy]):\n",
        "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Test Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['Test Loss'], [test_loss])\n",
        "plt.title('Test Loss')\n",
        "for i, v in enumerate([test_loss]):\n",
        "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ODPdpLqBYje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# F1 Score\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(['F1 Score'], [test_f1])\n",
        "plt.title('Test F1 Score')\n",
        "plt.ylim(0, 1)\n",
        "for i, v in enumerate([test_f1]):\n",
        "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['AUC'], [test_auc])\n",
        "plt.title('Test AUC')\n",
        "plt.ylim(0, 1)\n",
        "for i, v in enumerate([test_auc]):\n",
        "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ujv2UnWGYjj3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}